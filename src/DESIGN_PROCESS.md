Design Process
=============

Group members
--------------
Nivedita Chopra (niveditc)
Madeline Horowitz (mhorowit)
Isaac Lim (idl)
Naman Bharadwaj (nbharadw, not in 237, but approved by Prof. Kosbie)


Our design process
--------------
1) Brainstorming and storyboarding (see storyboards.pdf)
2) Planning out the database structure (we did this in a shared Evernote
   notebook)
3) Coding up both front-end clients (mobile+desktop), and the back-end
4) User tests (see below)
5) Incorporating feedback received from user tests (see below)
6) Bigger round of user tests
7) One more round of incorporating feedback received
8) Final touches before releasing public beta


Discussion on user tests
--------------
Near the completion of our project, we gave the link to our site to a small
group of friends, asking them to help us test it for correctness, stability,
and general bug searching. In order to better document their feedback, we
created a feedback form on Google Docs with questions specific to each client
(desktop and mobile). These questions were mainly metric evaluations such as
rating an aspect of the app between 1 and 5, yes/no responses, and so on. This
allowed us to see the perspectives of our potential users, in terms of what
worked and what didn’t.

Some results obtained are as follows (note that all ratings are out of 5).
About 60% of the responses rated the user-friendliness of our desktop interface
a 4, and the remaining 40% gave it a 5. We’re happy that our users seem to
like the user experience that we came up with, since this was one of our
initial goals for this project. We incorporated their feedback including things
like using dialogs for the course browser and event adder on mobile, instead
of having separate pages. On desktop, we replaced many icons and UI elements
that our testers complained about being unintuitive, among many other things.

Also, CourseEvents was relatively well-received, with 60% of the responders 
giving the usefulness of this feature a 4, and the rest a 3. This is an 
indication to us that this feature has potential, and that we have to put in
more work to really bring out its benefits.

Another set of responses that we received that was more of a mixed bag was to
the question “How compelled would you feel to switch to ScheduleCMU (from
ScheduleMan) once it's ready?” 40% gave this a 4, while the rest were evenly
distributed among the lower ratings. This rating is partially due to the fact
that this is still an initial beta of our app, and so bugs and the simplicity
of several features still take away from the full experience that we have in
mind for the final version.

Lastly, we also had several long-answer questions asking our users for their
opinions of the operation of various aspects of the app. These were extremely
well-answered and very useful to us to iteratively engineer the app. Another
one of our initial goals was to create this app specifically tailored to the
likes and dislikes of the CMU community. Thus tweaking the app according to
the feedback that we receive is of utmost priority to us.

We are due to open up ScheduleCMU as a public beta to everyone soon, once we
feel that it is ready to take on a larger group of testers. We hope to
continue receiving great feedback from this very welcoming community, in order
to continue building on our app.